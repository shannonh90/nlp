{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Practice with NLTK Chunking Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CONLL Corpus is tagged with NP, VP, and PP chunks and has a built in  training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def train_conll_chunker(chunk_type='NP'):\n",
    "    chunk_trees = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_type)\n",
    "    return (chunk_trees)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP This/DT)\n",
      "  has/VBZ\n",
      "  increased/VBN\n",
      "  (NP the/DT risk/NN)\n",
      "  of/IN\n",
      "  (NP the/DT government/NN)\n",
      "  being/VBG\n",
      "  forced/VBN\n",
      "  to/TO\n",
      "  increase/VB\n",
      "  (NP base/NN rates/NNS)\n",
      "  to/TO\n",
      "  (NP 16/CD %/NN)\n",
      "  from/IN\n",
      "  (NP their/PRP$ current/JJ 15/CD %/NN level/NN)\n",
      "  to/TO\n",
      "  defend/VB\n",
      "  (NP the/DT pound/NN)\n",
      "  ,/,\n",
      "  (NP economists/NNS)\n",
      "  and/CC\n",
      "  (NP foreign/JJ exchange/NN market/NN analysts/NNS)\n",
      "  say/VBP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# take a look at the output of the NP chunker\n",
    "np_trees = train_conll_chunker('NP')\n",
    "print(np_trees[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  (VP has/VBZ increased/VBN)\n",
      "  the/DT\n",
      "  risk/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  government/NN\n",
      "  (VP being/VBG forced/VBN to/TO increase/VB)\n",
      "  base/NN\n",
      "  rates/NNS\n",
      "  to/TO\n",
      "  16/CD\n",
      "  %/NN\n",
      "  from/IN\n",
      "  their/PRP$\n",
      "  current/JJ\n",
      "  15/CD\n",
      "  %/NN\n",
      "  level/NN\n",
      "  (VP to/TO defend/VB)\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  ,/,\n",
      "  economists/NNS\n",
      "  and/CC\n",
      "  foreign/JJ\n",
      "  exchange/NN\n",
      "  market/NN\n",
      "  analysts/NNS\n",
      "  (VP say/VBP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# take a look at the output of the NP chunker\n",
    "\n",
    "vp_trees = train_conll_chunker('VP')\n",
    "print(vp_trees[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate tracing by testing two chunkers with different rule ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NP chunker which puts NPs with adjectives first in the rule ordering\n",
    "cp1 = nltk.RegexpParser(r'''\n",
    "NP: {<DT><JJ.*><NN.*>} #Chunk det+adj+noun\n",
    "    {<DT|NN.*>+}      #Chunk sequences of DT and noun (any number of determiners)\n",
    "    ''')\n",
    "\n",
    "# NP chunker which puts NPs with adjectives second in the rule ordering\n",
    "cp2 = nltk.RegexpParser(r'''\n",
    "NP:   {<DT|NN.*>+}      #Chunk sequences of DT and noun\n",
    "     {<DT><JJ.*><NN.*>} #Chunk det+adj+noun\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('enchantress', 'NN'),\n",
       " ('clutched', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('hair', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(nltk.word_tokenize(\"The enchantress clutched the beautiful hair\"))\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output of cp1 and cp2, and watch the trace as it happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cp1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9946538723d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cp1' is not defined"
     ]
    }
   ],
   "source": [
    "print(cp1.parse(tagged_tokens, trace=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Input:\n",
      " <DT>  <NN>  <VBD>  <DT>  <JJ>  <NN> \n",
      "# Chunk sequences of DT and noun:\n",
      "{<DT>  <NN>} <VBD> {<DT>} <JJ> {<NN>}\n",
      "# Chunk det+adj+noun:\n",
      "{<DT>  <NN>} <VBD> {<DT>} <JJ> {<NN>}\n",
      "(S\n",
      "  (NP The/DT enchantress/NN)\n",
      "  clutched/VBD\n",
      "  (NP the/DT)\n",
      "  beautiful/JJ\n",
      "  (NP hair/NN))\n"
     ]
    }
   ],
   "source": [
    "print(cp2.parse(tagged_tokens, trace=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Why did these two outputs differ?###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Collocations Practice#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK bigram and trigram functions to create collocations and compute interesting ones using PMI ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import string, random\n",
    "\n",
    "text = \"pubmed_depression.txt\"\n",
    "#take sept 7 hw, load in your text, split by spaces\n",
    "\n",
    "# Analyze my own text\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('u', 'b'), 1),\n",
       " (('.', 't'), 1),\n",
       " (('t', 'x'), 1),\n",
       " (('p', 'u'), 1),\n",
       " (('p', 'r'), 1),\n",
       " (('e', 'p'), 1),\n",
       " (('i', 'o'), 1),\n",
       " (('x', 't'), 1),\n",
       " (('m', 'e'), 1),\n",
       " (('e', 's'), 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.ngram_fd.most_common(10)   #going straight into FD, look at most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'the'), 250),\n",
       " (('of', 'the'), 233),\n",
       " (('to', 'the'), 135),\n",
       " (('on', 'the'), 133),\n",
       " (('at', 'the'), 114),\n",
       " (('and', 'the'), 111),\n",
       " (('to', 'be'), 111),\n",
       " (('it', 'was'), 104),\n",
       " (('was', 'a'), 97),\n",
       " (('he', 'had'), 91)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove tokens that start with punctuation\n",
    "finder.apply_word_filter(lambda w: w[0] in string.punctuation)\n",
    "\n",
    "finder.ngram_fd.most_common(10) #dont have punc, but a lot of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('could', 'see'), 15),\n",
       " (('old', 'man'), 14),\n",
       " (('New', 'York'), 13),\n",
       " (('Mike', 'Deegan'), 12),\n",
       " (('Old', 'Man'), 12),\n",
       " ((\"I've\", 'got'), 11),\n",
       " (('young', 'men'), 9),\n",
       " (('Poor', 'John'), 9),\n",
       " ((\"didn't\", 'know'), 9),\n",
       " (('looked', 'like'), 9)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words)\n",
    "\n",
    "finder.ngram_fd.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A.M.', 'starring'),\n",
       " ('A40-AjK', 'Mercedes'),\n",
       " ('Air', 'Force'),\n",
       " ('Akita', 'prefectures'),\n",
       " ('Appian', 'Way'),\n",
       " ('Arc', 'de'),\n",
       " ('Armed', 'Forces'),\n",
       " ('Ash', 'Road'),\n",
       " ('Auto', 'Company'),\n",
       " (\"Best's\", 'Liliputian')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply pmi measure to rank the remaining bigrams\n",
    "finder.nbest(bigram_measures.pmi, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9476\n",
      "1626\n"
     ]
    }
   ],
   "source": [
    "# Those results aren't great.  To fix PMI, make sure bigrams occur at least 2 times\n",
    "print(finder.ngram_fd.N())\n",
    "finder.apply_freq_filter(2)\n",
    "print(finder.ngram_fd.N())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bobbsey', 'Twins'),\n",
       " ('Bon', 'jour'),\n",
       " ('Crazy', 'Horse'),\n",
       " ('Jour', 'et'),\n",
       " ('Mt.', 'Pleasant'),\n",
       " ('Signor', 'Raymond'),\n",
       " ('Toodle', 'Williams'),\n",
       " ('Unit', 'Number'),\n",
       " ('V-shaped', 'inlet'),\n",
       " ('Young', \"Christians'\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply pmi measure to rank the remaining bigrams\n",
    "finder.nbest(bigram_measures.pmi, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same thing for news text ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1,257,700', 'non-farm'),\n",
       " ('100-yard', 'dash'),\n",
       " ('1044', 'Chestnut'),\n",
       " ('11-7', 'collapse'),\n",
       " ('1200', 'Larimer'),\n",
       " ('13-5', 'barrage'),\n",
       " ('165-unit', 'Harbor'),\n",
       " ('1671', 'Nakoma'),\n",
       " ('182', 'scholastics'),\n",
       " ('2-and-2', 'pitches')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same thing for news text\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.brown.words(categories=\"news\"))\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w[0] in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words)\n",
    "finder.nbest(bigram_measures.chi_sq, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do trigram collocations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trigram collocations for romance: \n",
      "[('Jour', 'et', 'Nuit'), ('Chief', 'Crazy', 'Horse'), ('Young', \"Christians'\", 'League'), ('hundred', 'dollar', 'bill'), ('wet', 'graham', 'crackers'), ('Evadna', 'Mae', 'Evans'), ('tightly', 'curled', 'paot'), ('Dr.', 'Fortman', 'says'), ('Mrs.', 'Gertrude', 'Parker'), ('New', 'York', 'harbor'), ('Cousin', 'Alexander', 'Carraway'), ('Frankie', 'Ricco', 'sat'), ('Poor', 'Cousin', 'Elec'), ('inning', 'Mike', 'Deegan'), ('Miss', 'Theresa', 'Stubblefield'), ('Old', 'Mr.', 'Thom'), ('told', 'Miss', 'Groggins'), (\"isn't\", 'strong', 'enough'), ('said', 'Samuel', 'Burns'), ('Gratt', 'Shafer', 'would')]\n",
      "\n",
      "Trigram collocations for news: \n",
      "[('Ku', 'Klux', 'Klan'), ('Pinar', 'Del', 'Rio'), ('Rural', 'Roads', 'Authority'), ('Post', 'Office', 'Box'), ('Diversified', 'Growth', 'Stock'), (\"Patrick's\", 'Day', 'Purse'), ('Notre', 'Dame', 'Chapter'), ('Growth', 'Stock', 'Fund'), ('electronic', 'data', 'processing'), ('esprit', 'de', 'corps'), ('La', 'Dolce', 'Vita'), ('oil', 'mill', 'supplies'), ('Prince', 'Souvanna', 'Phouma'), ('Holy', 'Cross', 'Hospital'), ('First', 'Lady', 'Jacqueline'), ('Duncan', 'Phyfe', 'furniture'), ('Cherry', 'Hill', 'Road'), ('Dow', 'Jones', 'industrial'), ('Speaker', 'Sam', 'Rayburn'), ('J.', 'Clinton', 'Bowman')]\n"
     ]
    }
   ],
   "source": [
    "def trigram_collocations(words, num):\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    finder = TrigramCollocationFinder.from_words(words)\n",
    "    finder.apply_word_filter(lambda w: w[0] in string.punctuation)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stop_words)\n",
    "    finder.apply_freq_filter(2)\n",
    "    return(finder.nbest(bigram_measures.pmi, num))\n",
    "\n",
    "print(\"\\nTrigram collocations for romance: \")\n",
    "print(trigram_collocations(nltk.corpus.brown.words(categories=\"romance\"), 20))\n",
    "\n",
    "print(\"\\nTrigram collocations for news: \")\n",
    "print(trigram_collocations(nltk.corpus.brown.words(categories=\"news\"), 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
